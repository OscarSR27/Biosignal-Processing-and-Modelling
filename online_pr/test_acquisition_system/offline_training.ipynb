{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nProject: P300: Speller Matrix\\nAuthors:\\n    -Natascha Niessen\\n    -Pamela Reyna\\n    -Oscar Soto\\n    -Alena Starikova\\nDescription: Offline analysis and visualization of training EEG data for a P300 application\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Project: P300: Speller Matrix\n",
    "Authors:\n",
    "    -Natascha Niessen\n",
    "    -Pamela Reyna\n",
    "    -Oscar Soto\n",
    "    -Alena Starikova\n",
    "Description: Offline analysis and visualization of training EEG data for a P300 application\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyxdf\n",
    "import mne\n",
    "import joblib\n",
    "from mne.datasets import misc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read NPY file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-pass filter from 2 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 2.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 1.00 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 825 samples (1.650 s)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19501,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "#Load npy files\n",
    "eeg_samples = np.transpose(np.load('eeg_samples.npy'))\n",
    "eeg_timestamps = np.load('eeg_timestamps.npy')\n",
    "marker_samples = np.load('marker_samples.npy')\n",
    "marker_timestamps = np.load('marker_timestamps.npy')\n",
    "corrected_marker_timestamps = np.load('corrected_marker_timestamps.npy')\n",
    "\n",
    "eeg_data = mne.filter.filter_data(eeg_samples.astype(np.float64),500,2,16)\n",
    "print(np.shape(eeg_timestamps))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an mne object in order to use filter and epoch functions. Extract the elements from the dictiornaries.\n",
    "streams[0] contains info about markers and streams[1] contains info about EEG data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates RAW data object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stream 2: Calculated effective sampling rate 8.8342 Hz is different from specified rate 500.0000 Hz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of streams found: 2\n"
     ]
    }
   ],
   "source": [
    "#When reading an XDF file you have streams. In our case, each stream corresponds to an outlet recorded in lab recorder. \n",
    "#However, the order in which those structures are saved in the xdf file is not always the same. To identify it, it is \n",
    "#necessary to consult the 'type' key as shown below\n",
    "xdf_file_path = \"data/sub-P001_ses-S200_task-Default_run-001_eeg.xdf\"\n",
    "streams, header = pyxdf.load_xdf(xdf_file_path)\n",
    "for stream in range(len(streams)): #This for loop will iterate over all the streams we have. Normally we only have 2, the EEG signal and the markers. However, we parameterize it so that the code is reusable.\n",
    "    type = streams[stream]['info']['type'][0]\n",
    "    if type == 'EEG':\n",
    "        eeg_stream_idx = stream\n",
    "    if type == 'Markers':\n",
    "        marker_stream_idx = stream\n",
    "        \n",
    "print(\"Number of streams found: \" + str(len(streams)))\n",
    "\n",
    "eeg_data_raw = streams[eeg_stream_idx]['time_series']\n",
    "sampling_rate = streams[eeg_stream_idx]['info']['nominal_srate'][0]\n",
    "\n",
    "#Extract channel names\n",
    "number_channels = np.shape(streams[eeg_stream_idx]['info']['desc'][0]['channels'][0]['channel'])[0]\n",
    "ch_names = []\n",
    "ch_pos = {}\n",
    "for i in range(number_channels):\n",
    "    channel_label = streams[eeg_stream_idx]['info']['desc'][0]['channels'][0]['channel'][i]['label'][0]\n",
    "    pos_x = np.float64(streams[eeg_stream_idx]['info']['desc'][0]['channels'][0]['channel'][i]['location'][0]['X'][0])#Get X channel position\n",
    "    pos_y = np.float64(streams[eeg_stream_idx]['info']['desc'][0]['channels'][0]['channel'][i]['location'][0]['Y'][0])#Get Y channel position\n",
    "    pos_z = np.float64(streams[eeg_stream_idx]['info']['desc'][0]['channels'][0]['channel'][i]['location'][0]['Z'][0])#Get Z channel position\n",
    "    \n",
    "    ch_names.append(channel_label)\n",
    "    ch_pos[channel_label] = [pos_x,pos_y,pos_z]\n",
    "   \n",
    "ch_types = ['eeg'] * len(ch_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create events to use for the epoching phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mne.tools/stable/generated/mne.Epochs.html\n",
    "marker_id = np.squeeze(marker_samples)\n",
    "marker_timestamps_scale =  marker_timestamps - eeg_timestamps[0]\n",
    "\n",
    "#Creates event array, for this we need event_sample and event_id. Our event sample will be int(marker_timestamps[i]) and event_id as define below\n",
    "events = []\n",
    "for i,marker in enumerate(marker_id):\n",
    "    event_id = {'Init': 10, 'StartTask': 11, 'End': 12, 'S0': 0, 'S1': 1, 'S2': 2}[marker] #Iterates over marker_id and every time it finds an S10 it assigns a 1 to event_id, \n",
    "                                                  #every time it finds an S11 it assigns a 2 to event_id list and then append the value to\n",
    "                                                  #events\n",
    "    #events.append([int((marker_timestamps[i]-6.889)*1000), 0, event_id])\n",
    "    events.append([marker_timestamps_scale[i], 0, event_id])\n",
    "                                          \n",
    "#Why do we use numeric values ​​instead of directly using S10 or S11? \n",
    "# Some functions can expect number values ​​as parameters, so it is more convenient to use a dictionary and assign \n",
    "# S10 and S11 a numeric representation. In case we need to use a string, it is easier to pass this number value \n",
    "# to a string or use the dictionary to index its key (S10 or S11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events with the same timestamps:\n"
     ]
    }
   ],
   "source": [
    "duplicate_events = []\n",
    "timestamps = [event[0] for event in events]\n",
    "\n",
    "for i in range(len(events)):\n",
    "    if timestamps.count(events[i][0]) > 1:\n",
    "        duplicate_events.append(i)\n",
    "\n",
    "print(\"Events with the same timestamps:\")\n",
    "for index in duplicate_events:\n",
    "    print(events[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 19501)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(eeg_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=24, n_times=19501\n",
      "    Range : 0 ... 19500 =      0.000 ...    39.000 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ohskr\\AppData\\Local\\Temp\\ipykernel_30344\\3522202612.py:5: RuntimeWarning: Fiducial point nasion not found, assuming identity unknown to head transformation\n",
      "  info.set_montage(montage)\n"
     ]
    }
   ],
   "source": [
    "# Create info object\n",
    "info = mne.create_info(ch_names, sfreq=500, ch_types=ch_types)\n",
    "#Set channel positions\n",
    "montage = mne.channels.make_dig_montage(ch_pos)\n",
    "info.set_montage(montage)\n",
    "\n",
    "raw = mne.io.RawArray(eeg_data, info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot EEG signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib as 2D backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MNEBrowseFigure size 1707x897 with 4 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels marked as bad:\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "raw.plot(events=np.asarray(events),scalings=dict(eeg=1e1),show_options=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 4.096 (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ohskr\\AppData\\Local\\Temp\\ipykernel_30344\\547714129.py:1: RuntimeWarning: in version 1.5, the default behavior of Spectrum.plot() will change so that bad channels will be shown by default. To keep the old default behavior (and silence this warning), explicitly pass `picks='data', exclude='bads'`.\n",
      "  raw.compute_psd(fmax = 30).plot()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MNELineFigure size 1000x350 with 2 Axes>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.compute_psd(fmax = 30).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement and call functions for epoching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_indices(arr1, arr2):\n",
    "    indices = []\n",
    "    for value in arr1:\n",
    "        abs_diff = np.abs(arr2 - value)\n",
    "        nearest_index = np.argmin(abs_diff) #The minimum absolute value between the arrays is calculated. For example if arr2 has elements 454 and 456 and value is 455, \n",
    "                                            #the subtraction abs(454-455) = 1 and abs(456-455) = 1, how then select only one of them? np.argmin() returns only the index of \n",
    "                                            #the first minimum found in case there are several minimum values ​​in the array.\n",
    "        indices.append(nearest_index)\n",
    "    return indices\n",
    "\n",
    "def epoch(raw,marker_timestamps,marker_id,marker,tmin,tmax,sampling_rate):\n",
    "    #Let's go and find how many samples tmin and tmax are equal to. Remember: Samples are integers/discrete values, thats why we have to cast to int\n",
    "    tmin_samples = np.abs(int(tmin*int(sampling_rate)))\n",
    "    tmax_samples = np.abs(int(tmax*int(sampling_rate)))\n",
    "    marker_index = np.where(marker_id == marker)\n",
    "    time_points = np.round(marker_timestamps[np.where(marker_id == marker)],3)\n",
    "    indices = find_nearest_indices(time_points,raw.times)\n",
    "    epochs = []\n",
    "    eeg_data = np.array(raw.get_data())\n",
    "    for trial in range(len(indices)):\n",
    "        epochs.append(eeg_data[:,indices[trial] - tmin_samples:indices[trial] + tmax_samples])\n",
    "    \n",
    "    epochs = np.array(epochs)#Convert to numpy array\n",
    "    return epochs, marker_index\n",
    "\n",
    "tmin = -0.5\n",
    "tmax = 1\n",
    "epochsS0,S0_idx = epoch(raw,marker_timestamps_scale,marker_id,'S0',tmin,tmax,500)\n",
    "epochsS1,S1_idx = epoch(raw,marker_timestamps_scale,marker_id,'S1',tmin,tmax,500)\n",
    "epochsS2,S2_idx = epoch(raw,marker_timestamps_scale,marker_id,'S2',tmin,tmax,500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(tmin,tmax,1/int(500)) #This is not the general time but an snapshop from -0.500 to 1.000 using the sample sampling frequency used in data collection\n",
    "S0_avg = np.mean(epochsS0,axis = 0)\n",
    "S1_avg = np.mean(epochsS1,axis = 0)\n",
    "S2_avg = np.mean(epochsS2,axis = 0)\n",
    "\n",
    "#Select a window to look for maximum\n",
    "lower_time_window = 0.250\n",
    "upper_time_window = 0.305\n",
    "lower_idx = np.where(time >= lower_time_window)[0][0]\n",
    "upper_idx = np.where(time <= upper_time_window)[0][-1]\n",
    "\n",
    "#Set plot setting (title, number of subplots, etc)\n",
    "fig, axs = plt.subplots(int(len(ch_names)/4), int(len(ch_names)/6))\n",
    "fig.suptitle(\"Epoch avg for event and non event trials\")\n",
    "fig.subplots_adjust(hspace=0.9)\n",
    "plot_next_row = 0\n",
    "plot_next_column = 0\n",
    "#Create plots\n",
    "for ch in range(int(len(ch_names))):\n",
    "    if ch%6 == 0 and ch != 0: #Each 6 rows update the indexes for subplots\n",
    "        plot_next_row = 0\n",
    "        plot_next_column = plot_next_column + 1 \n",
    "    axs[plot_next_row, plot_next_column].plot(time,S0_avg[ch,:], color = 'lightgreen',label=\"S0\")\n",
    "    axs[plot_next_row, plot_next_column].plot(time,S1_avg[ch,:], color = 'blue',label=\"S1\")\n",
    "    axs[plot_next_row, plot_next_column].plot(time,S2_avg[ch,:], color = 'red',label=\"S2\")\n",
    "    \n",
    "    #Create one legend for all plots\n",
    "    if (plot_next_row == 0 and plot_next_column == 0):\n",
    "        legend_labels = ['S0', 'S1', 'S2']\n",
    "        axs[plot_next_row, plot_next_column].legend(labels=legend_labels, loc='upper center', bbox_to_anchor=(-0.4, 1), ncol=1)\n",
    "    \n",
    "    #Add titles, time value and set grid\n",
    "    axs[plot_next_row, plot_next_column].set_title('Ch ' + str(ch + 1) + ': ' + ch_names[ch])#Title\n",
    "    axs[plot_next_row, plot_next_column].grid(True)#grid on\n",
    "    \n",
    "    plot_next_row = plot_next_row + 1\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Online method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los arreglos son diferentes.\n"
     ]
    }
   ],
   "source": [
    "# Comparar si los arreglos son iguales\n",
    "are_equal = np.array_equal(eeg_timestamps_marker_S0, S0_idx)\n",
    "\n",
    "# Imprimir el resultado\n",
    "if are_equal:\n",
    "    print(\"Los arreglos son iguales.\")\n",
    "else:\n",
    "    print(\"Los arreglos son diferentes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15500 16004 16006 16500 16504 16506 17002 17005 17007 17500 17505 17506\n",
      " 18000 18005 18500]\n",
      "[16031, 16318, 16433, 16605, 16835, 16949, 17179, 17351, 17465, 17580, 17867, 17924, 18096, 18385, 18558]\n"
     ]
    }
   ],
   "source": [
    "print(eeg_timestamps_marker_S0)\n",
    "print(S0_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-pass filter from 2 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 2.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 1.00 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 4.00 Hz (-6 dB cutoff frequency: 18.00 Hz)\n",
      "- Filter length: 825 samples (1.650 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "low_bandpass_fr = 2\n",
    "high_bandpass_fr = 16\n",
    "eeg_samples_filter = mne.filter.filter_data(eeg_samples.astype(np.float64),sampling_rate,low_bandpass_fr,high_bandpass_fr)\n",
    "#To do the epoching we will first split the marker_sample array according to its \n",
    "# marker, S0, S1 or S2. By finding the indices of each one, we can use them to \n",
    "# index their corresponding timestamps. That is, obtain the times in which each \n",
    "# marker occurred. Having this information, we only have to search for these times \n",
    "# within the eeg_samples timestamp, obtain their indices and then use them \n",
    "# to finally index the eeg_samples \n",
    "markers_S0_idx = np.where(marker_samples == \"S0\")[0]\n",
    "markers_S1_idx = np.where(marker_samples == \"S1\")[0]\n",
    "markers_S2_idx = np.where(marker_samples == \"S2\")[0]\n",
    "\n",
    "timestamps_S0 = corrected_marker_timestamps[markers_S0_idx]\n",
    "timestamps_S1 = corrected_marker_timestamps[markers_S1_idx]\n",
    "timestamps_S2 = corrected_marker_timestamps[markers_S2_idx]\n",
    "\n",
    "eeg_timestamps_marker_S0 = np.where(np.isin(eeg_timestamps, timestamps_S0))[0]\n",
    "eeg_timestamps_marker_S1 = np.where(np.isin(eeg_timestamps, timestamps_S1))[0]\n",
    "eeg_timestamps_marker_S2 = np.where(np.isin(eeg_timestamps, timestamps_S2))[0]\n",
    "\n",
    "game_trials = len(eeg_timestamps_marker_S0)\n",
    "\n",
    "tmin = 0\n",
    "tmax = 1\n",
    "tmin_samples = np.abs(int(tmin*int(sampling_rate)))\n",
    "tmax_samples = np.abs(int(tmax*int(sampling_rate)))\n",
    "\n",
    "epochs_S0 = []\n",
    "epochs_S1 = []\n",
    "epochs_S2 = []\n",
    "for game_trial in range(game_trials):\n",
    "    epochs_S0.append(eeg_samples_filter[:,eeg_timestamps_marker_S0[game_trial]:eeg_timestamps_marker_S0[game_trial] + tmax_samples])\n",
    "    epochs_S1.append(eeg_samples_filter[:,eeg_timestamps_marker_S1[game_trial]:eeg_timestamps_marker_S1[game_trial] + tmax_samples])\n",
    "    epochs_S2.append(eeg_samples_filter[:,eeg_timestamps_marker_S2[game_trial]:eeg_timestamps_marker_S2[game_trial] + tmax_samples])\n",
    "\n",
    "epochs_S0 = np.array(epochs_S0)\n",
    "epochs_S1 = np.array(epochs_S1)\n",
    "epochs_S2 = np.array(epochs_S2)\n",
    "\n",
    "avg_marker_S0 = np.mean(epochs_S0, axis = 0)\n",
    "avg_marker_S1 = np.mean(epochs_S1, axis = 0)\n",
    "avg_marker_S2 = np.mean(epochs_S2, axis = 0)\n",
    "\n",
    "#Select a window to look for maximum and get the indexes\n",
    "time = np.arange(tmin,tmax,1/int(sampling_rate))\n",
    "lower_time_window = 0.250\n",
    "upper_time_window = 0.305\n",
    "lower_idx = np.where(time >= lower_time_window)[0][0]\n",
    "upper_idx = np.where(time <= upper_time_window)[0][-1]\n",
    "\n",
    "plt.plot(time,avg_marker_S0[21,:],color='lightgreen')\n",
    "plt.plot(time,avg_marker_S1[21,:],color='blue')\n",
    "plt.plot(time,avg_marker_S2[21,:],color='red')\n",
    "legend_labels = ['0', '1', '2']\n",
    "plt.legend(legend_labels)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_per_task_for_event = 15\n",
    "trials_per_task_for_non_event = 30\n",
    "number_of_tasks = 30\n",
    "number_of_trials_event = number_of_tasks*trials_per_task_for_event\n",
    "number_of_trials_non_event = number_of_tasks*trials_per_task_for_non_event\n",
    "\n",
    "_,number_of_ch,number_of_samples = np.shape(event_epochs)\n",
    "\n",
    "feature_method = \"Method 1\"\n",
    "\n",
    "feature_methods = {\n",
    "    \"Method 1\": {\n",
    "        \"number\": 1,\n",
    "        \"description\": \"The average of each trial for each task is calculated, and then the maximum value is extracted from each average within the range \" + str(lower_time_window)+\"s:\"+str(upper_time_window) +\"s.\"\n",
    "    },\n",
    "    \"Method 2\": {\n",
    "        \"number\": 2,\n",
    "        \"description\": \"The maximum value for each trial is extracted within the range \" + str(lower_time_window)+\"s:\"+str(upper_time_window) +\"s.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    feature_method_selection = feature_methods[feature_method][\"number\"]\n",
    "    if(feature_method_selection == 1):\n",
    "        description = feature_methods[feature_method][\"description\"]\n",
    "        print(\"Description:\", description)\n",
    "        #Average trial per task (event markers)\n",
    "        task_counter = 0\n",
    "        event_avg_per_trial_per_task = np.zeros((number_of_tasks, number_of_ch, number_of_samples))\n",
    "        feature_1_event = np.zeros((number_of_tasks,number_of_ch))\n",
    "        for trial_group_event in range(0,number_of_trials_event - 1,trials_per_task_for_event):\n",
    "            event_avg_per_trial_per_task[task_counter,:,:] = np.mean(event_epochs[trial_group_event:(trial_group_event+trials_per_task_for_event-1),:,:],axis = 0)#Average event trials per task\n",
    "            feature_1_event[task_counter,:] = np.max(event_avg_per_trial_per_task[task_counter, :, lower_idx:upper_idx], axis=1)#Get the maximum amplitude for each average event trial per task in the given range and store it in the feature vector\n",
    "            task_counter = task_counter + 1\n",
    "\n",
    "        feature_1_event = np.array(feature_1_event)\n",
    "\n",
    "        #Average trial per task (non event markers)\n",
    "        task_counter = 0\n",
    "        non_event_avg_per_trial_per_task = np.zeros((number_of_tasks, number_of_ch, number_of_samples))\n",
    "        feature_1_non_event = np.zeros((number_of_tasks,number_of_ch))\n",
    "        for trial_group_non_event in range(0,number_of_trials_non_event - 1,trials_per_task_for_non_event):\n",
    "            non_event_avg_per_trial_per_task[task_counter,:,:] = np.mean(non_event_epochs[trial_group_non_event:(trial_group_non_event+trials_per_task_for_non_event-1),:,:],axis = 0)#Average event trials per task\n",
    "            feature_1_non_event[task_counter,:] = np.max(non_event_avg_per_trial_per_task[task_counter, :, lower_idx:upper_idx], axis=1)#Get the maximum amplitude for each average non event trial per task in the given range and store it in the feature vector\n",
    "            task_counter = task_counter + 1\n",
    "\n",
    "        feature_1_non_event = np.array(feature_1_non_event)\n",
    "\n",
    "        #Create labels for event and non event class\n",
    "        labels_event = np.full((len(feature_1_event), 1), 1).astype(int)\n",
    "        labels_non_event = np.full((len(feature_1_non_event), 1), -1).astype(int)\n",
    "\n",
    "        #Concatenate vectors in a single training data matrix\n",
    "        features = np.concatenate((feature_1_event, feature_1_non_event), axis=0)\n",
    "        labels = np.concatenate((labels_event, labels_non_event), axis=0)\n",
    "        training_data = np.concatenate((features, labels), axis=1)\n",
    "    elif (feature_method_selection == 2):\n",
    "        description = feature_methods[feature_method][\"description\"]\n",
    "        print(\"Description:\", description)\n",
    "        feature_1_event = np.zeros((number_of_trials_event,number_of_ch))\n",
    "        for trial in range(number_of_trials_event):\n",
    "            feature_1_event[trial,:] = np.max(event_epochs[trial, :, lower_idx:upper_idx], axis=1)#Get the maximum amplitude for each trial in the given range and store it in the feature vector\n",
    "\n",
    "        feature_1_event = np.array(feature_1_event)\n",
    "\n",
    "        feature_1_non_event = np.zeros((number_of_trials_non_event,number_of_ch))\n",
    "        for trial in range(number_of_trials_non_event):\n",
    "            feature_1_non_event[trial,:] = np.max(non_event_epochs[trial, :, lower_idx:upper_idx], axis=1)#Get the maximum amplitude for each trial in the given range and store it in the feature vector\n",
    "\n",
    "        feature_1_non_event = np.array(feature_1_non_event)\n",
    "\n",
    "        #Create labels for event and non event class\n",
    "        labels_event = np.full((len(feature_1_event), 1), 1).astype(int)\n",
    "        labels_non_event = np.full((len(feature_1_non_event), 1), -1).astype(int)\n",
    "\n",
    "        #Concatenate vectors in a single training data matrix\n",
    "        features = np.concatenate((feature_1_event, feature_1_non_event), axis=0)\n",
    "        labels = np.concatenate((labels_event, labels_non_event), axis=0)\n",
    "        training_data = np.concatenate((features, labels), axis=1)\n",
    "except:\n",
    "    print(\"That feature extraction method is not valid\")\n",
    "    for method, details in feature_methods.items():\n",
    "        print(method, \"Description:\", details[\"description\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (feature_method == \"Method 1\"):\n",
    "    time = np.arange(tmin,tmax,1/int(sampling_rate))\n",
    "\n",
    "    for ch in range(len(ch_names)):\n",
    "        \n",
    "        #Set plot setting (title, number of subplots, etc)\n",
    "        fig, axs = plt.subplots(int(number_of_tasks/3), int(number_of_tasks/10))\n",
    "        fig.suptitle(\"Avg trial per task channel \" + str(ch + 1) + ': ' + ch_names[ch])\n",
    "        fig.subplots_adjust(hspace=0.5)\n",
    "        plot_next_row = 0\n",
    "        plot_next_column = 0\n",
    "        \n",
    "        #Create plots\n",
    "        for task in range(number_of_tasks):\n",
    "            max_idx = np.where(event_avg_per_trial_per_task[task, ch, :] == feature_1_event[task,ch])[0][0]\n",
    "            if task%3 == 0 and task != 0: #Each 6 rows update the indexes for subplots\n",
    "                plot_next_column = 0\n",
    "                plot_next_row = plot_next_row + 1\n",
    "            axs[plot_next_row, plot_next_column].plot(time,event_avg_per_trial_per_task[task,ch,:], color = 'lightgreen',label=\"Event\")\n",
    "            axs[plot_next_row, plot_next_column].plot(time,non_event_avg_per_trial_per_task[task,ch,:], color = 'blue',label=\"Non Event\")\n",
    "            \n",
    "            #Create one legend for all plots\n",
    "            if (plot_next_row == 0 and plot_next_column == 0):\n",
    "                legend_labels = ['Event', 'Non Event']\n",
    "                axs[plot_next_row, plot_next_column].legend(labels=legend_labels, loc='upper center', bbox_to_anchor=(-0.35, 1.2), ncol=1)\n",
    "            \n",
    "            #Add markers at maximum time points in the given range (lower_idx:upper_idx)\n",
    "            axs[plot_next_row, plot_next_column].plot(time[max_idx], event_avg_per_trial_per_task[task,ch,max_idx], marker='x', color='red')\n",
    "            axs[plot_next_row, plot_next_column].plot(time[max_idx], non_event_avg_per_trial_per_task[task,ch,max_idx], marker='x', color='red')\n",
    "            \n",
    "            #Add titles, time value and set grid\n",
    "            axs[plot_next_row, plot_next_column].set_title('Task ' + str(task + 1), fontsize=8)#Title\n",
    "            axs[plot_next_row, plot_next_column].text(time[max_idx], 0, str(np.round(time[max_idx],3)), fontsize=9, color='black', ha='center', va='bottom')#Time value\n",
    "            axs[plot_next_row, plot_next_column].grid(True)#grid on\n",
    "            \n",
    "            plot_next_column = plot_next_column + 1\n",
    "            \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Visualization is not implemented for this method\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisherrank(featv, labels):\n",
    "    nfeat = featv.shape[1]\n",
    "    c1 = featv[labels == -1, :]\n",
    "    c2 = featv[labels == 1, :]\n",
    "    \n",
    "    d = np.zeros(nfeat)\n",
    "    \n",
    "    for f in range(nfeat):\n",
    "        d[f] = ((np.mean(c1[:, f]) - np.mean(c2[:, f]))**2) / (np.var(c1[:, f]) + np.var(c2[:, f]))\n",
    "    \n",
    "    rank = np.argsort(d)[::-1]\n",
    "    \n",
    "    return d, rank\n",
    "\n",
    "d, rank = fisherrank(training_data[:,:-1], training_data[:,-1])\n",
    "#d, rank = fisherrank(normalize(training_data[:,:-1],axis = 1), training_data[:,-1])\n",
    "criterion_desc = np.sort(d)[::-1]\n",
    "\n",
    "# Plot criterion_desc\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(criterion_desc)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Ranked Features')\n",
    "plt.ylabel('Fisher Score')\n",
    "\n",
    "# Scatter plot\n",
    "event_length = len(feature_1_event)\n",
    "non_event_length = len(feature_1_non_event)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(training_data[:event_length - 1, rank[0]], training_data[:event_length - 1, rank[1]], color='blue')\n",
    "plt.scatter(training_data[event_length:event_length+non_event_length - 1, rank[0]], training_data[event_length:event_length+non_event_length - 1, rank[1]], color='red')\n",
    "plt.xlabel(\"1. Best Feature\")\n",
    "plt.ylabel(\"2. Best Feature\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials, _ = np.shape(training_data)\n",
    "\n",
    "# Select the best features in such a way that there are at least 10 times more observations than features\n",
    "number_of_features_to_pick = 0.1*trials\n",
    "features_to_pick_idx = np.arange(number_of_features_to_pick).astype(int)\n",
    "\n",
    "#Save best features index in txt file so we can use it for online data\n",
    "file_path = '../online_pr/trained_model/best_features_idx.txt'\n",
    "np.savetxt(file_path, rank[features_to_pick_idx], fmt='%d')\n",
    "\n",
    "#Create a new feature vector with best features for training\n",
    "best_features = np.concatenate((training_data[:,rank[features_to_pick_idx]], np.reshape(training_data[:,-1],(-1,1))), axis=1)\n",
    "\n",
    "# Cross validation\n",
    "k_folds = 10\n",
    "\n",
    "# Set ratio for test set\n",
    "test_ratio = 0.1\n",
    "\n",
    "# We are going to do several repetitions of the cross-validation to have a\n",
    "# better estimate of the performance\n",
    "repetitions = 100\n",
    "\n",
    "MCR_LDA_validation_acc = 0\n",
    "MCR_SVM_validation_acc = 0\n",
    "MCR_LDA_test_acc = 0\n",
    "MCR_SVM_test_acc = 0\n",
    "\n",
    "# In the following algorithm we will perform an evaluation of the performance of the classifier.\n",
    "# In order to have more accurate results we will use the cross-validation technique with kfolds = 10.\n",
    "# In addition, we will repeat the entire algorithm 100 times to calculate the average performance\n",
    "# for both the validation set and the test set.\n",
    "for repetition in range(repetitions):\n",
    "    # Get a randomly permuted array of indices\n",
    "    shuffled_indices = np.random.permutation(best_features.shape[0])\n",
    "\n",
    "    # Shuffle the rows of the array using the shuffled indices\n",
    "    random_features = best_features[shuffled_indices, :]\n",
    "\n",
    "    #Normalization\n",
    "    norm_features = random_features[:,:-1]\n",
    "    #norm_features = normalize(random_features[:,:-1],axis=0)\n",
    "    \n",
    "    # Generate random indices from 0 to rows for test set\n",
    "    random_test_idx = np.random.choice(trials-1, size=int(trials*test_ratio), replace=False)\n",
    "    \n",
    "    # Get the indices for training set (this will later be divided into training and validation set)\n",
    "    random_training_idx = np.setdiff1d(np.arange(trials), random_test_idx)\n",
    "\n",
    "    features_training_set_aux = norm_features[random_training_idx]\n",
    "    labels_training_set_aux = random_features[random_training_idx,-1]#norm_features just contains features and no labels thats why we use random_features here, that has the same order than norm_features\n",
    "    features_test_set = norm_features[random_test_idx]\n",
    "    labels_test_set = random_features[random_test_idx,-1]#norm_features just contains features and no labels thats why we use random_features here, that has the same order than norm_features\n",
    "\n",
    "    MCR_accumulator_LDA = 0\n",
    "    MCR_accumulator_SVM = 0\n",
    "\n",
    "    # Cross-validation\n",
    "    cvIndices = KFold(n_splits=k_folds).split(features_training_set_aux)\n",
    "    for i in range(k_folds):\n",
    "\n",
    "        train_indices, validation_indices = next(cvIndices)\n",
    "\n",
    "        features_training_set = features_training_set_aux[train_indices]\n",
    "        labels_training_set = labels_training_set_aux[train_indices]\n",
    "        features_validation_set = features_training_set_aux[validation_indices]\n",
    "        labels_validation_set = labels_training_set_aux[validation_indices]\n",
    "\n",
    "        # Train and validate an LDA model for this kfold\n",
    "        lda_model = LinearDiscriminantAnalysis()\n",
    "        lda_model.fit(features_training_set, labels_training_set)\n",
    "        validation_LDA = lda_model.predict(features_validation_set)\n",
    "\n",
    "        # Train and validate a SVM model for this kfold\n",
    "        svm_model = SVC()\n",
    "        svm_model.fit(features_training_set, labels_training_set)\n",
    "        validation_SVM = svm_model.predict(features_validation_set)\n",
    "\n",
    "        # Misclassification rate (MCR)\n",
    "        misclassification_LDA = np.sum(validation_LDA != labels_validation_set)\n",
    "        misclassification_SVM = np.sum(validation_SVM != labels_validation_set)\n",
    "\n",
    "        # Store the validation performance for this kfold\n",
    "        MCR_LDA = misclassification_LDA / len(labels_validation_set)\n",
    "        MCR_SVM = misclassification_SVM / len(labels_validation_set)\n",
    "\n",
    "        MCR_accumulator_LDA = MCR_accumulator_LDA + MCR_LDA\n",
    "        MCR_accumulator_SVM = MCR_accumulator_SVM + MCR_SVM\n",
    "\n",
    "    # Store the validation performance for this repetition\n",
    "    MCR_Avg_LDA = MCR_accumulator_LDA / k_folds\n",
    "    MCR_LDA_validation_acc = MCR_LDA_validation_acc + MCR_Avg_LDA\n",
    "\n",
    "    MCR_Avg_SVM = MCR_accumulator_SVM / k_folds\n",
    "    MCR_SVM_validation_acc = MCR_SVM_validation_acc + MCR_Avg_SVM\n",
    "\n",
    "    # Test the models\n",
    "    test_LDA = lda_model.predict(features_test_set)\n",
    "    test_SVM = svm_model.predict(features_test_set)\n",
    "\n",
    "    # Store the test performance for this repetition\n",
    "    MCR_LDA_test_repetition = np.sum(test_LDA != labels_test_set) / len(labels_test_set)\n",
    "    MCR_LDA_test_acc = MCR_LDA_test_acc + MCR_LDA_test_repetition\n",
    "\n",
    "    MCR_SVM_test_repetition = np.sum(test_SVM != labels_test_set) / len(labels_test_set)\n",
    "    MCR_SVM_test_acc = MCR_SVM_test_acc + MCR_SVM_test_repetition\n",
    "\n",
    "# Show average performance for validation and test sets\n",
    "MCR_LDA_validation = MCR_LDA_validation_acc / repetitions\n",
    "print(f'LDA Model: Average misclassification rate (MCR) for validation set is {MCR_LDA_validation}')\n",
    "\n",
    "MCR_SVM_validation = MCR_SVM_validation_acc / repetitions\n",
    "print(f'SVM Model: Average misclassification rate (MCR) for validation set is {MCR_SVM_validation}')\n",
    "\n",
    "MCR_LDA_test = MCR_LDA_test_acc / repetitions\n",
    "print(f'LDA Model: Misclassification rate (MCR) for test set is {MCR_LDA_test}')\n",
    "\n",
    "MCR_SVM_test = MCR_SVM_test_acc / repetitions\n",
    "print(f'SVM Model: Misclassification rate (MCR) for test set is {MCR_SVM_test}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose model and train classifier with full training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a randomly permuted array of indices\n",
    "shuffled_indices = np.random.permutation(best_features.shape[0])\n",
    "\n",
    "# Shuffle the rows of the array using the shuffled indices\n",
    "random_features = best_features[shuffled_indices, :]\n",
    "\n",
    "#Normalization\n",
    "norm_features = random_features[:,:-1]\n",
    "#norm_features = normalize(random_features[:,:-1],axis=0)\n",
    "\n",
    "#Train model with full training data set\n",
    "#svm_model = SVC()\n",
    "#svm_model.fit(norm_features, random_features[:,-1])\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(norm_features, random_features[:,-1])\n",
    "\n",
    "#Save model for future use in online sessions\n",
    "file_path = '../online_pr/trained_model/lda_model.pkl'\n",
    "joblib.dump(lda_model, file_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test classifier with unseen data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and process file\n",
    "#xdf_file_path = \"data/sub-P001_ses-S101_task-Default_run-001_eeg.xdf\"\n",
    "#xdf_file_path = \"data/sub-P001_ses-S200_task-Default_run-001_eeg.xdf\"\n",
    "#xdf_file_path = \"data/sub-P001_ses-S201_FaceDelay_task-Default_run-001_eeg.xdf\"\n",
    "xdf_file_path = \"data/sub-P001_ses-S202_NoBlackSBetweenTrials_task-Default_run-001_eeg.xdf\"\n",
    "#xdf_file_path = \"data/sub-P001_ses-S204_Just_Highlight_Interest_Number_task-Default_run-001_eeg.xdf\"\n",
    "streams, header = pyxdf.load_xdf(xdf_file_path)\n",
    "\n",
    "#When reading an XDF file you have streams. In our case, each stream corresponds to an outlet recorded in lab recorder. \n",
    "#However, the order in which those structures are saved in the xdf file is not always the same. To identify it, it is \n",
    "#necessary to consult the 'type' key as shown below\n",
    "\n",
    "for stream in range(len(streams)): #This for loop will iterate over all the streams we have. Normally we only have 2, the EEG signal and the markers. However, we parameterize it so that the code is reusable.\n",
    "    type = streams[stream]['info']['type'][0]\n",
    "    if type == 'EEG':\n",
    "        eeg_stream_idx = stream\n",
    "    if type == 'Markers':\n",
    "        marker_stream_idx = stream\n",
    "        \n",
    "print(\"Number of streams found: \" + str(len(streams)))\n",
    "\n",
    "#Creates RAW data object\n",
    "eeg_data_raw = streams[eeg_stream_idx]['time_series']\n",
    "sampling_rate = streams[eeg_stream_idx]['info']['nominal_srate'][0]\n",
    "eeg_data = mne.filter.filter_data(eeg_data_raw.T.astype(np.float64),sampling_rate,2,16)\n",
    "eeg_timestamps = streams[eeg_stream_idx]['time_stamps']\n",
    "\n",
    "#Extract channel names\n",
    "number_channels = np.shape(streams[eeg_stream_idx]['info']['desc'][0]['channels'][0]['channel'])[0]\n",
    "ch_names = []\n",
    "ch_pos = {}\n",
    "for i in range(number_channels):\n",
    "    channel_label = streams[eeg_stream_idx]['info']['desc'][0]['channels'][0]['channel'][i]['label'][0]\n",
    "    pos_x = np.float64(streams[eeg_stream_idx]['info']['desc'][0]['channels'][0]['channel'][i]['location'][0]['X'][0])#Get X channel position\n",
    "    pos_y = np.float64(streams[eeg_stream_idx]['info']['desc'][0]['channels'][0]['channel'][i]['location'][0]['Y'][0])#Get Y channel position\n",
    "    pos_z = np.float64(streams[eeg_stream_idx]['info']['desc'][0]['channels'][0]['channel'][i]['location'][0]['Z'][0])#Get Z channel position\n",
    "    \n",
    "    ch_names.append(channel_label)\n",
    "    ch_pos[channel_label] = [pos_x,pos_y,pos_z]\n",
    "   \n",
    "ch_types = ['eeg'] * len(ch_names)\n",
    "\n",
    "#Create events to use for the epoching phase\n",
    "#https://mne.tools/stable/generated/mne.Epochs.html\n",
    "marker_id = np.squeeze(streams[marker_stream_idx]['time_series'])\n",
    "marker_timestamps = streams[marker_stream_idx]['time_stamps']\n",
    "marker_timestamps =  marker_timestamps - eeg_timestamps[0]\n",
    "\n",
    "#Creates event array, for this we need event_sample and event_id. Our event sample will be int(marker_timestamps[i]) and event_id as define below\n",
    "events = []\n",
    "for i,marker in enumerate(marker_id):\n",
    "    event_id = {'S10': 1, 'S11': 2}[marker] #Iterates over marker_id and every time it finds an S10 it assigns a 1 to event_id, \n",
    "                                                  #every time it finds an S11 it assigns a 2 to event_id list and then append the value to\n",
    "                                                  #events\n",
    "    #events.append([int((marker_timestamps[i]-6.889)*1000), 0, event_id])\n",
    "    events.append([marker_timestamps[i], 0, event_id])\n",
    "                                          \n",
    "#Why do we use numeric values ​​instead of directly using S10 or S11? \n",
    "# Some functions can expect number values ​​as parameters, so it is more convenient to use a dictionary and assign \n",
    "# S10 and S11 a numeric representation. In case we need to use a string, it is easier to pass this number value \n",
    "# to a string or use the dictionary to index its key (S10 or S11)\n",
    "\n",
    "# Create info object\n",
    "info = mne.create_info(ch_names, sfreq=sampling_rate, ch_types=ch_types)\n",
    "#Set channel positions\n",
    "montage = mne.channels.make_dig_montage(ch_pos)\n",
    "info.set_montage(montage)\n",
    "\n",
    "raw = mne.io.RawArray(eeg_data, info)\n",
    "\n",
    "#Epoching\n",
    "event_epochs = epoch(raw,marker_timestamps,marker_id,'S10',tmin,tmax,sampling_rate)\n",
    "non_event_epochs = epoch(raw,marker_timestamps,marker_id,'S11',tmin,tmax,sampling_rate)\n",
    "\n",
    "#Feature extraction\n",
    "\n",
    "try:\n",
    "    feature_method_selection = feature_methods[feature_method][\"number\"]\n",
    "    if(feature_method_selection == 1):\n",
    "        description = feature_methods[feature_method][\"description\"]\n",
    "        print(\"Description:\", description)\n",
    "        #Average trial per task (event markers)\n",
    "        task_counter = 0\n",
    "        event_avg_per_trial_per_task = np.zeros((number_of_tasks, number_of_ch, number_of_samples))\n",
    "        feature_1_event = np.zeros((number_of_tasks,number_of_ch))\n",
    "        for trial_group_event in range(0,number_of_trials_event - 1,trials_per_task_for_event):\n",
    "            event_avg_per_trial_per_task[task_counter,:,:] = np.mean(event_epochs[trial_group_event:(trial_group_event+trials_per_task_for_event-1),:,:],axis = 0)#Average event trials per task\n",
    "            feature_1_event[task_counter,:] = np.max(event_avg_per_trial_per_task[task_counter, :, lower_idx:upper_idx], axis=1)#Get the maximum amplitude for each average event trial per task in the given range and store it in the feature vector\n",
    "            task_counter = task_counter + 1\n",
    "\n",
    "        feature_1_event = np.array(feature_1_event)\n",
    "\n",
    "        #Average trial per task (non event markers)\n",
    "        task_counter = 0\n",
    "        non_event_avg_per_trial_per_task = np.zeros((number_of_tasks, number_of_ch, number_of_samples))\n",
    "        feature_1_non_event = np.zeros((number_of_tasks,number_of_ch))\n",
    "        for trial_group_non_event in range(0,number_of_trials_non_event - 1,trials_per_task_for_non_event):\n",
    "            non_event_avg_per_trial_per_task[task_counter,:,:] = np.mean(non_event_epochs[trial_group_non_event:(trial_group_non_event+trials_per_task_for_non_event-1),:,:],axis = 0)#Average event trials per task\n",
    "            feature_1_non_event[task_counter,:] = np.max(non_event_avg_per_trial_per_task[task_counter, :, lower_idx:upper_idx], axis=1)#Get the maximum amplitude for each average non event trial per task in the given range and store it in the feature vector\n",
    "            task_counter = task_counter + 1\n",
    "\n",
    "        feature_1_non_event = np.array(feature_1_non_event)\n",
    "\n",
    "        #Create labels for event and non event class\n",
    "        labels_event = np.full((len(feature_1_event), 1), 1).astype(int)\n",
    "        labels_non_event = np.full((len(feature_1_non_event), 1), -1).astype(int)\n",
    "\n",
    "        #Concatenate vectors in a single training data matrix\n",
    "        features = np.concatenate((feature_1_event, feature_1_non_event), axis=0)\n",
    "        labels = np.concatenate((labels_event, labels_non_event), axis=0)\n",
    "        training_data = np.concatenate((features, labels), axis=1)\n",
    "    elif (feature_method_selection == 2):\n",
    "        description = feature_methods[feature_method][\"description\"]\n",
    "        print(\"Description:\", description)\n",
    "        feature_1_event = np.zeros((number_of_trials_event,number_of_ch))\n",
    "        for trial in range(number_of_trials_event):\n",
    "            feature_1_event[trial,:] = np.max(event_epochs[trial, :, lower_idx:upper_idx], axis=1)#Get the maximum amplitude for each trial in the given range and store it in the feature vector\n",
    "\n",
    "        feature_1_event = np.array(feature_1_event)\n",
    "\n",
    "        feature_1_non_event = np.zeros((number_of_trials_non_event,number_of_ch))\n",
    "        for trial in range(number_of_trials_non_event):\n",
    "            feature_1_non_event[trial,:] = np.max(non_event_epochs[trial, :, lower_idx:upper_idx], axis=1)#Get the maximum amplitude for each trial in the given range and store it in the feature vector\n",
    "\n",
    "        feature_1_non_event = np.array(feature_1_non_event)\n",
    "\n",
    "        #Create labels for event and non event class\n",
    "        labels_event = np.full((len(feature_1_event), 1), 1).astype(int)\n",
    "        labels_non_event = np.full((len(feature_1_non_event), 1), -1).astype(int)\n",
    "\n",
    "        #Concatenate vectors in a single training data matrix\n",
    "        features = np.concatenate((feature_1_event, feature_1_non_event), axis=0)\n",
    "        labels = np.concatenate((labels_event, labels_non_event), axis=0)\n",
    "        training_data = np.concatenate((features, labels), axis=1)\n",
    "except:\n",
    "    print(\"That feature extraction method is not valid\")\n",
    "    for method, details in feature_methods.items():\n",
    "        print(method, \"Description:\", details[\"description\"])\n",
    "\n",
    "#Get best features base on fisher rank during training phase\n",
    "best_features = np.concatenate((training_data[:,rank[features_to_pick_idx]], np.reshape(training_data[:,-1],(-1,1))), axis=1)\n",
    "\n",
    "# Get a randomly permuted array of indices\n",
    "shuffled_indices = np.random.permutation(best_features.shape[0])\n",
    "\n",
    "# Shuffle the rows of the array using the shuffled indices\n",
    "random_features = best_features[shuffled_indices, :]\n",
    "\n",
    "#Normalization\n",
    "norm_features = random_features[:,:-1]\n",
    "#norm_features = normalize(random_features[:,:-1],axis=0)\n",
    "\n",
    "#Predict labels and compare with true labels\n",
    "#test_SVM = svm_model.predict(norm_features)\n",
    "#MCR_SVM_test_unseen = np.sum(test_SVM != random_features[:,-1]) / len(random_features[:,-1])\n",
    "#print(f'SVM Model: Misclassification rate (MCR) for unseen set is {MCR_SVM_test_unseen}')\n",
    "\n",
    "test_LDA = lda_model.predict(norm_features)\n",
    "MCR_LDA_test_unseen = np.sum(test_LDA != random_features[:,-1]) / len(random_features[:,-1])\n",
    "print(f'LDA Model: Misclassification rate (MCR) for unseen set is {MCR_LDA_test_unseen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
